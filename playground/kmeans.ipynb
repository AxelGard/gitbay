{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN for search "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load data and clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from html.parser import HTMLParser\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def load_data():\n",
    "    PATH = \"../data/websites/\"\n",
    "    contents = []\n",
    "    files = [f\"{PATH}{f}\" for f in listdir(PATH) if isfile(join(PATH, f))]\n",
    "    for file_path in files:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            contents.append(f.read())\n",
    "    return files, contents\n",
    "\n",
    "\n",
    "def load_stop_words(path: str = \"../stopwords.txt\"):\n",
    "    words = []\n",
    "    with open(path, \"r\") as f:\n",
    "        words = f.read().split(\"\\n\")\n",
    "    assert len(words) != 0, \"no stop words were found\"\n",
    "    return words\n",
    "\n",
    "\n",
    "def clean_html(contents:list) -> list:\n",
    "    result = []\n",
    "    for html in contents:\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        result.append(soup.get_text())\n",
    "    return result\n",
    "\n",
    "def clean(contents: list, remove_stop_words=True):\n",
    "    contents = clean_html(contents)\n",
    "    ascii_char = [chr(i) for i in range(0, 255)]\n",
    "    numbers = \"0123456789\"\n",
    "    non_acc_char = \"\\n,.()[]{}`/:-_*=\\\\<>|&%@?!\\\"'#\" + numbers\n",
    "    non_acc_tokens = [\"https\", \"www\", \"com\", \"org\", \"license\"]\n",
    "    stop_words = [\"\"] \n",
    "    if remove_stop_words: \n",
    "        stop_words = load_stop_words()\n",
    "    for i, _ in enumerate(contents):\n",
    "        for c in non_acc_char:\n",
    "            contents[i] = contents[i].replace(c, \" \")\n",
    "        contents[i] = contents[i].split(\" \")\n",
    "        contents[i] = list(filter(lambda c: c != \"\", contents[i]))\n",
    "        contents[i] = [t for t in contents[i] if not t in non_acc_tokens]\n",
    "        contents[i] = [\n",
    "            s.lower() for s in contents[i] if all(c in ascii_char for c in s)\n",
    "        ]\n",
    "        contents[i] = [t for t in contents[i] if not t in stop_words]\n",
    "        for j, word in enumerate(contents[i]):\n",
    "            if word[-1] == \"s\":\n",
    "                contents[i][j] = word[:-1]\n",
    "    return [\" \".join(con) for con in contents]\n",
    "\n",
    "\n",
    "def words_to_vec(words: str, labels: dict = {}):\n",
    "    _words = words.split(\" \")\n",
    "    vec = []\n",
    "    for word in _words:\n",
    "        if word not in labels:\n",
    "            labels[word] = len(labels)\n",
    "        vec.append(labels[word])\n",
    "    return vec, labels\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### impl knn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
